---
title: "Homework8"
author: "Patrick Seebold"
format: pdf
editor: visual
---

This homework will explore basics of predictive modeling using an online-available data set concerning bike data in Seoul, South Korea.

First, let's get our libraries set:

```{r, echo = FALSE}
library(tidymodels)
library(lubridate)
library(dplyr)
library(ggplot2)
```

## EDA

Next, let's load the data in and resolve the known error mentioned in the instructions. We can specify the fileEncoding and avoid checking names to solve the problem:

```{r}
data = read.csv("SeoulBikeData.csv", fileEncoding = "Latin1", check.names = F)
```

Great, now we can go ahead with our EDA to make sure everything looks good. First, lets check for missing values:

```{r}
sum(is.na(data))
```

Sweet, no missing values. makes for a good start. Next, we'll take a look at the column types, rename to avoid strange variable names, and change our categorical variables to factors. The instructions say to change our variable names later in the process, but I'd rather they be in a clean format for the rest of the EDA!

```{r}
summary(data)
colnames(data) = c('Date', 'NumBikes', "Hour", "Temperature", "Humidity",
                   "WindSpeed","Visibility", "DewPointTemp", "SolarRad", "Rainfall"
                   , "Snowfall", "Season", "Holiday", "FunctioningDay" )

data$Season = as.factor(data$Season)
data$Holiday = as.factor(data$Holiday)
data$FunctioningDay = as.factor(data$FunctioningDay)
levels(data$Season)
levels(data$Holiday)
levels(data$FunctioningDay)
```

Great, now we can see that everything looks good with our numerical variables, our variable names are no longer likely to cause an issue, and our categorical variables are appropriately recast as factors! Next, we will change the date into a workable arithmetic form using lubridate:

```{r}
typeof(data$Date) # currently character

data$Date = lubridate::dmy(data$Date)
typeof(data$Date) # now it's a double!
```

Now that we have cleaned the data up, we will do some summary stats, specifically looking at bike rental count, rainfall, and snowfall. We'll also examine these across some categorical variables, such as FunctioningDay, Holiday, and Season:

```{r}
data |> # check bike numbers by season
  group_by(FunctioningDay) |>
  summarize(mean = mean(NumBikes), sd = sd(NumBikes))# Non-functioning days mean no bikes!

sum(data$FunctioningDay == "No") # 295 of the data points can be excluded

sub_data = subset(data, data$FunctioningDay == "Yes") # we can ignore days that are not functioning


sub_data |> # check bike numbers by season
  group_by(Season) |>
  summarize(mean = mean(NumBikes), sd = sd(NumBikes))

sub_data |> # check bike numbers by holiday
  group_by(Holiday) |>
  summarize(mean = mean(NumBikes), sd = sd(NumBikes))

sub_data |> # check rain  by season
  group_by(Season) |>
  summarize(mean = mean(Rainfall),  sd = sd(Rainfall))

sub_data |> # check snowfall by season
  group_by(Season) |>
  summarize(mean = mean(Snowfall),  sd = sd(Snowfall))
```

We see that summer appears to have the highest number of bike rentals, and winter has the fewest. Bike rentals are more common on Non-Holidays, so they are likely being used to commute to work. Finally, we see from our tables that snowfall is most plentiful in winter, while rainfall is most plentiful in summer.

Next up, let's summarize across hours so that we can collapse each day into a single observation.

```{r}
clean_data = sub_data |>
  group_by(Date, Season, Holiday) |>
  summarize(TotBikes = sum(NumBikes), TotRain = sum(Rainfall), TotSnow = sum(Snowfall), MeanTemp = mean(Temperature), MeanHumid = mean(Humidity), MeanWindSpeed = mean(WindSpeed), MeanVis = mean(Visibility), MeanDewPoint = mean(DewPointTemp), MeanSolar = mean(SolarRad))
head(clean_data)
  
```

Great, now we have our final data set for training. Let's recreate our basic summaries and do some plots on this cleaned data. We'll also report some correlations. Let's first recreate our summary stats:

```{r}

clean_data |> # check bike numbers by season
  group_by(Season) |>
  summarize(mean = mean(TotBikes), sd = sd(TotBikes))

clean_data |> # check bike numbers by holiday
  group_by(Holiday) |>
  summarize(mean = mean(TotBikes), sd = sd(TotBikes))

clean_data |> # check rain  by season
  group_by(Season) |>
  summarize(mean = mean(TotRain),  sd = sd(TotRain))

clean_data |> # check snowfall by season
  group_by(Season) |>
  summarize(mean = mean(TotSnow),  sd = sd(TotSnow))
```

We see that the same trends hold; more bikes in summer, more bikes on non-holidays, more rain in summer, and more snow in winter. Next, let's plot some of the numerical variables:

```{r}
ggplot(clean_data, aes(x = TotRain, y = TotBikes)) +
       geom_point() +
       labs(x = 'Total Rain', y = 'Total Bikes', title = 'Bike Rentals by Amount of rain')

ggplot(clean_data, aes(x = TotSnow, y = TotBikes)) +
       geom_point() +
       labs(x = 'Total Snow', y = 'Total Bikes', title = 'Bike Rentals by Amount of snow')

ggplot(clean_data, aes(x = MeanTemp, y = TotBikes)) +
       geom_point() +
       geom_smooth() +
       labs(x = 'Mean Temperature', y = 'Total Bikes', title = 'Bike Rentals by Temperature')
```

These plots make sense. We see a lot more bike rentals when there is low snow and low rain. Also, we see that the number of bikes increase as temperature increases, until a certain point. These plots appear to be accurately capturing info about how weather influence bike riding behavior!

Finally, let's plot some correlations:

```{r}
cor(clean_data[sapply(clean_data, is.numeric)])
```

We can see some impressive correlations here. For example, Mean Temp, Mean Dew Point, and Mean Solar are all quite positively correlated with total number of bike rentals. These are likely all tied into the weather we commented on above - when the weather is nice, people are more likely to rent bikes, thus leading to higher correlation values! We should be able to do some nice prediction on this data given the strengths of these correlations, although we might be at risk of overfitting if we aren't careful.

## Modeling the Data

Now we can prep our model for prediction! First, let's make our test and training sets:

```{r}
set.seed(42)
splits = initial_split(clean_data, prop = 0.75, strata = "Season")
train = training(splits)
test = testing(splits)

head(train)
nrow(train)
head(test)
nrow(test)
```

Everything appears to be in order with our test and training sets, and we have stratified by Season as requested. Next up, we'll make three recipes, train our three MLR models, and then compare their performances:

```{r}
# All three of our recipes will use the same recipes. However, model 1 will only include all variables as predictors, while models 2 and 3 will include more complex terms.

# Recipe 1: all numeric variables, no interactions
bike_rec1 = recipe(TotBikes ~., data = clean_data) |>
  step_date(Date, features = "dow") |>
  step_mutate(DayType = factor(if_else(wday(Date) %in% c(1,7), "Weekend", "Weekday"), levels = c("Weekend","Weekday"))) |>
  step_normalize(all_numeric_predictors()) |>
  step_dummy(Season, Holiday, DayType) |>
  step_rm(Date_dow, Date) #|>
  #prep(training = clean_data) |>
  #bake(clean_data) # let's make sure it worked as planned!

# Recipe 2: all numeric variables, some interactions
bike_rec2 = recipe(TotBikes ~., data = clean_data) |>
  step_date(Date, features = "dow") |>
  step_mutate(DayType = factor(if_else(wday(Date) %in% c(1,7), "Weekend", "Weekday"), levels = c("Weekend","Weekday"))) |>
  step_normalize(all_numeric_predictors()) |>
  step_dummy(Season, Holiday, DayType) |>
  step_rm(Date_dow, Date) |>
  step_interact(~ Season_Spring:Holiday_No.Holiday +
                  Season_Summer:Holiday_No.Holiday +
                  Season_Winter:Holiday_No.Holiday +
                  Season_Spring:TotRain + Season_Summer:TotRain +
                  Season_Winter:TotRain + 
                  MeanTemp:TotRain)

# Recipe 3: all numeric variables, some interacitons, & quad term
bike_rec3 = recipe(TotBikes ~., data = clean_data) |>
  step_date(Date, features = "dow") |>
  step_mutate(DayType = factor(if_else(wday(Date) %in% c(1,7), "Weekend", "Weekday"), levels = c("Weekend","Weekday"))) |>
  step_normalize(all_numeric_predictors()) |>
  step_dummy(Season, Holiday, DayType) |>
  step_rm(Date_dow, Date) |>
  step_interact(~ Season_Spring:Holiday_No.Holiday +
                  Season_Summer:Holiday_No.Holiday +
                  Season_Winter:Holiday_No.Holiday +
                  Season_Spring:TotRain + Season_Summer:TotRain +
                  Season_Winter:TotRain + 
                  MeanTemp:TotRain) |>
  step_poly(TotRain, TotSnow, MeanTemp, MeanHumid, MeanWindSpeed,
            MeanVis, MeanDewPoint , MeanSolar, degree = 2)
```

After prepping and baking the data, we can see that everything seems to be in order! Now, we can declare our model and set up our workflow:

```{r}
bike_mod = linear_reg() %>%
  set_engine("lm")

bike_wfl1 = workflow() |>
  add_recipe(bike_rec1) |>
  add_model(bike_mod)
             
bike_wfl2 = workflow() |>
  add_recipe(bike_rec2) |>
  add_model(bike_mod)

bike_wfl3 = workflow() |>
  add_recipe(bike_rec3) |>
  add_model(bike_mod)
```

Cool, now we have the workflows for each of our models ready to! Let's set up the 10-fold cross validation next:

```{r}
bike_cv10 = vfold_cv(clean_data, 10) # same CV for all models
set.seed(42)

bike_cv_fits1 = bike_wfl1 |>
  fit_resamples(bike_cv10)
bike_cv_fits1 |>
  collect_metrics()

bike_cv_fits2 = bike_wfl2 |>
  fit_resamples(bike_cv10)
bike_cv_fits2 |>
  collect_metrics()

bike_cv_fits3 = bike_wfl3 |>
  fit_resamples(bike_cv10)
bike_cv_fits3 |>
  collect_metrics()
```

After fitting all three models, we see that the first one has the lowest RMSE. Also, this first model is the simplest of the set! We will use recipe 1 on the full training set and then use collect_metrics() to find the test RMSE:

```{r}
final_mod = last_fit(bike_wfl1, split = splits)
clean_final_mod = collect_metrics(final_mod)
clean_final_mod
```

And finally, we can obtain the final model coefficient table:

```{r}
final_fits = extract_fit_parsnip(final_mod)
final_coef = tidy(final_fits)
final_coef
```

And there we have it! We can see how the various different parameters are expected to impact bike rentals in this specific town, and could use these to predict how well our bikes may fair depending on the weather, season, and other variables.

------------------------------------------------------------------------

# Homework 9 New Material

------------------------------------------------------------------------

For this portion of HW9, we will fit 4 new models: a tuned LASSO, a tuned Regression Tree, a tuned Bagged Tree, and a tuned Random Forest. We'll do them in this order, then compare the best tuned version of each to the MLR model from HW8. We'll report model fittings, and then train our best model on the full data set!

## Tuned LASSO Model

Recall that our data is already split as follows, which we will use for these new models too:

> splits = initial_split(clean_data, prop = 0.75, strata = "Season")
>
> train = training(splits)
>
> test = testing(splits)
>
> bike_cv10 = vfold_cv(clean_data, 10)

```{r}
# let's throw all of our numeric predictors into the LASSO, so that it can effectively help us 'pick' which ones are worth including!
set.seed(42)
LASSO_rec = bike_rec1 # we can use the same basic recipe as with our MLR from last time - this entails all predictors in the cleaned data set

# tune the penalty term, and specify LASSO instead of elastic net
LASSO_spec = linear_reg(penalty=tune(), mixture = 1) |>
  set_engine("glmnet")

LASSO_wkf = workflow() |>
  add_recipe(LASSO_rec)|>
  add_model(LASSO_spec)

LASSO_grid = LASSO_wkf |>
  tune_grid(resamples = bike_cv10,
            grid = grid_regular(penalty(), levels = 200))

lowest_rmse = LASSO_grid |>
  select_best(metric = 'rmse')

LASSO_final = LASSO_wkf |>
  finalize_workflow(lowest_rmse) |>
  fit(train) # we will use tidy(LASSO_final) late to produce the coefficients table

LASSO_wkf |>
  finalize_workflow(lowest_rmse) |>
  last_fit(splits, metrics = metric_set(rmse,mae)) |>
  collect_metrics()

# produce the coefs as requested
tidy(LASSO_final) 

# also produce the coefs for MLR model 1
final_coef
```

We see that the values of errors values are around RMSE = 3837 and MAE = 3117. This LASSO example is a great example of the power of this model workflow - we were able to easily recycle the recipe from our earlier project and effectively construct another type of model with ease! Our coefficient tables let us see how the MLR and LASSO models differ - it's interesting to note that temperature has been reduced to 0 in the LASSO model, yet temperature is one of the most important variables in our Bagged model!

## Tuned Regression Tree Model

Again, we can recycle the recipe from our earlier model for the sake of our regression tree:

```{r}
set.seed(42)
RegTree_rec = bike_rec1

RegTree_mod = decision_tree(tree_depth = tune(), min_n = 20, cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("regression")

RegTree_wkf = workflow() |>
  add_recipe(RegTree_rec) |>
  add_model(RegTree_mod)

#RegTree_wkf |> tune_grid(resamples = #bike_cv10) |>
#  collect_metrics()

RegTree_grid = grid_regular(cost_complexity(), tree_depth(), levels = c(10,5))

RegTree_fits = RegTree_wkf |>
  tune_grid(resamples = bike_cv10, grid = RegTree_grid)


RegTree_best_params = select_best(RegTree_fits, metric = "rmse")

RegTree_final_wkf = RegTree_wkf |>
  finalize_workflow(RegTree_best_params)

RegTree_final_fit = RegTree_final_wkf |>
  last_fit(splits,metrics = metric_set(rmse,mae))

RegTree_final_fit |> 
  collect_metrics()

# plot the final fit
RegTree_final_fit %>%
  extract_fit_engine() %>%
  rpart.plot::rpart.plot(roundint = FALSE)
```

We see that the MAE for this model is lower than for the LASSO, but our RMSE is higher. The diagram is hard to read because the font is so small, but is shows how we are making our decisions as we move down the branches!

## Tuned Bagged Tree model

Next up, let's run the bagged tree model:

```{r}
library(baguette)
set.seed(42)
Bag_rec = bike_rec1 # recycle the same recipe

Bag_spec = bag_tree(tree_depth = tune(), min_n = 10, cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("regression")

Bag_wkf = workflow() |>
  add_recipe(Bag_rec) |>
  add_model(Bag_spec)

Bag_grid = grid_regular(cost_complexity(), tree_depth(), levels = c(10,5))
  
Bag_fit = Bag_wkf |>
  tune_grid(resamples = bike_cv10, grid = Bag_grid)
  
Bag_best_params = select_best(Bag_fit, metric = 'rmse')
  
Bag_final_wkf = Bag_wkf |>
  finalize_workflow(Bag_best_params)

Bag_final_fit = Bag_final_wkf |>
  last_fit(splits, metrics = metric_set(rmse,mae))

Bag_final_fit |> 
  collect_metrics()

# variable importance chart for the bagged model:
Bag_final_model = extract_fit_engine(Bag_final_fit)
Bag_final_model$imp |>
  mutate(term = factor(term, levels = term)) |>
  ggplot(aes(x = term, y = value)) +
  geom_bar(stat ="identity") +
  coord_flip()
```

In this case, we again see that our MAE is lower than our RMSE! Furthermore, the Test RMSE for our bagged model is better than either of the previous models. Right now, RMSE = 3434 is the number to beat for our next model. Also, we can see that it looks like the temperature and dew point are both the most important variables for this model! This is interesting to see, since our LASSO model pushed temperature to zero.

## Tuned Random Forest

Finally, let's run our tuned Random Forest. This is similar to the Bagged model, but a bit different:

```{r}
library(ranger)
set.seed(42)
RF_rec = bike_rec1 # recycle the same recipe

RF_spec = rand_forest(mtry = tune()) |>
  set_engine("ranger") |>
  set_mode("regression")

RF_wkf = workflow() |>
  add_recipe(RF_rec) |>
  add_model(RF_spec)

# grid is simple enough that we don't need to specify it in advance
RF_fit = RF_wkf |> 
  tune_grid(resamples = bike_cv10, grid = 7)
  
RF_best_params = select_best(RF_fit, metric = 'rmse')
  
RF_final_wkf = RF_wkf |>
  finalize_workflow(RF_best_params)

RF_final_fit = RF_final_wkf |>
  last_fit(splits, metrics = metric_set(rmse,mae))

RF_final_fit |> 
  collect_metrics()

# variable importance chart for the RF Model:
RF_final_model =  RF_final_fit$.workflow[[1]]$fit
var_importance <- RF_final_model$variable.importance

  extract_fit_engine(RF_final_fit)
RF_final_model$imp |>
  mutate(term = factor(term, levels = term)) |>
  ggplot(aes(x = term, y = value)) +
  geom_bar(stat ="identity") +
  coord_flip()''
```

We have a winner! The Test RMSE for our random forest is 3343, the lowest that we have yet seen including the MLR models from HW8! The variable importance plots indicates that –
